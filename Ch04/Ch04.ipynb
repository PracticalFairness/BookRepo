{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True) \n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "            import load_preproc_data_adult\n",
    "from common_utils import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## utility functions\n",
    "from common_utils import compute_metrics ## taken from AIF360 github repo\n",
    "\n",
    "## for Reweighting\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing \n",
    "\n",
    "## Optimized Preprocessing\n",
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions\\\n",
    "            import get_distortion_adult\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "\n",
    "## Learning Fair Representations\n",
    "from aif360.algorithms.preprocessing.lfr import LFR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## p 113\n",
    "priv_group   = [{'sex': 1}]\n",
    "unpriv_group = [{'sex': 0}]\n",
    "census_data  = load_preproc_data_adult(['sex']) ## utility function to collapse categories\n",
    "                                                ## according to details of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions: splitting data, building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p 113\n",
    "def split_data_trn_vld_tst(data_raw):\n",
    "    dset_raw_trn, dset_raw_vt  = data_raw.split(   [0.7], shuffle = True)\n",
    "    dset_raw_vld, dset_raw_tst = dset_raw_vt.split([0.5], shuffle = True)\n",
    "    \n",
    "    return dset_raw_trn, dset_raw_vld, dset_raw_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p 121\n",
    "def build_logit_model(dset_trn, dset_tst, privileged_groups, unprivileged_groups):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_trn  = scaler.fit_transform(dset_trn.features)\n",
    "    y_trn  = dset_trn.labels.ravel()\n",
    "    w_trn  = dset_trn.instance_weights.ravel()\n",
    "    \n",
    "    lmod = LogisticRegression()\n",
    "    lmod.fit(X_trn, y_trn, \n",
    "             sample_weight = w_trn)\n",
    "\n",
    "    dset_tst_pred        = dset_tst.copy(deepcopy=True)\n",
    "    X_tst                = scaler.transform(dset_tst_pred.features)\n",
    "    dset_tst_pred.labels = lmod.predict(X_tst)\n",
    "    \n",
    "    print(\"HOMEMADE METRICS\")\n",
    "    priv_idx   = np.where(dset_tst_pred.protected_attributes.ravel() == 1.0)[0]\n",
    "    unpriv_idx = np.where(dset_tst_pred.protected_attributes.ravel() == 0.0)[0]\n",
    "    \n",
    "    print(np.sum(dset_tst_pred.labels[priv_idx] == 1.0) / \n",
    "          np.sum(dset_tst_pred.labels[priv_idx] > -1.0))\n",
    "    print(np.sum(dset_tst_pred.labels[unpriv_idx] == 1.0) / \n",
    "          np.sum(dset_tst_pred.labels[unpriv_idx] > -1.0))\n",
    "    print(\"Mean difference: %0.2f\" % \n",
    "          (np.mean(dset_tst_pred.labels[unpriv_idx]) - np.mean(dset_tst_pred.labels[priv_idx])))\n",
    "    print(\"Disparate impact: %0.2f\" % \n",
    "          (np.mean(dset_tst_pred.labels[unpriv_idx]) / np.mean(dset_tst_pred.labels[priv_idx])))\n",
    "\n",
    "    \n",
    "    metric_tst = BinaryLabelDatasetMetric(dset_tst_pred, unprivileged_groups, privileged_groups)\n",
    "    print(\"PREROLLED METRICS\")\n",
    "    print(metric_tst.num_positives(privileged = True) / metric_tst.num_instances(privileged = True))\n",
    "    print(metric_tst.num_positives(privileged = False) / metric_tst.num_instances(privileged = False))\n",
    "    print(\"Disparate impact is %0.2f (closer to 1 is better)\" % metric_tst.disparate_impact())\n",
    "    print(\"Mean difference  is %0.2f (closer to 0 is better)\" % metric_tst.mean_difference())\n",
    "\n",
    "    return lmod, dset_tst_pred, metric_tst "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine raw dataset and a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p 113\n",
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "# split into train, validate, test\n",
    "dset_raw_trn, dset_raw_vld, dset_raw_tst = split_data_trn_vld_tst(census_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p 113\n",
    "## calculate the metric of interest\n",
    "metric_raw_trn = BinaryLabelDatasetMetric(dset_raw_trn, \n",
    "                                         unprivileged_groups = unpriv_group,\n",
    "                                         privileged_groups   = priv_group)\n",
    "\n",
    "print(\"Disparate impact is   %0.2f (closed to 1 is better)\" % metric_raw_trn.disparate_impact())\n",
    "print(\"Mean difference is   %0.2f (closer to 0 is better)\" % metric_raw_trn.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taking a look at coefficient values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_raw_trn.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "## raw training data\n",
    "raw_lmod, raw_pred, raw_metric = build_logit_model(dset_raw_trn, dset_raw_tst, priv_group, unpriv_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot coefficients for a quick visual summary of values\n",
    "print(dset_raw_trn.feature_names[:9])\n",
    "plt.plot(raw_lmod.coef_.ravel()[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset_raw_trn.feature_names[9:])\n",
    "plt.plot(raw_lmod.coef_.ravel()[9:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppression\n",
    "### p 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logit_model_suppression(dset_trn, \n",
    "                                  dset_tst, \n",
    "                                  privileged_groups, \n",
    "                                  unprivileged_groups):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_trn  = scaler.fit_transform(dset_trn.features[:, 2:])\n",
    "    y_trn  = dset_trn.labels.ravel()\n",
    "    w_trn  = dset_trn.instance_weights.ravel()\n",
    "    \n",
    "    lmod = LogisticRegression()\n",
    "    lmod.fit(X_trn, y_trn, \n",
    "             sample_weight = w_trn)\n",
    "\n",
    "    dset_tst_pred        = dset_tst.copy(deepcopy=True)\n",
    "    X_tst                = scaler.transform(dset_tst_pred.features[:, 2:])\n",
    "    dset_tst_pred.labels = lmod.predict(X_tst)\n",
    "\n",
    "    metric_tst = BinaryLabelDatasetMetric(dset_tst_pred,\n",
    "                                          unprivileged_groups, \n",
    "                                          privileged_groups)\n",
    "    print(\"HOMEMADE METRICS\")\n",
    "    priv_idx = np.where(dset_tst_pred.protected_attributes.ravel() == 1.0)[0]\n",
    "    unpriv_idx = np.where(dset_tst_pred.protected_attributes.ravel() == 0.0)[0]\n",
    "    print(np.sum(dset_tst_pred.labels[priv_idx] == 1.0) / np.sum(dset_tst_pred.labels[priv_idx] > -1.0))\n",
    "    print(np.sum(dset_tst_pred.labels[unpriv_idx] == 1.0) / np.sum(dset_tst_pred.labels[unpriv_idx] > -1.0))\n",
    "    print(\"Mean difference: %0.2f\" % (np.mean(dset_tst_pred.labels[unpriv_idx]) - np.mean(dset_tst_pred.labels[priv_idx])))\n",
    "    print(\"Disparate impact: %0.2f\" % (np.mean(dset_tst_pred.labels[unpriv_idx]) / np.mean(dset_tst_pred.labels[priv_idx])))\n",
    "\n",
    "    \n",
    "    metric_tst = BinaryLabelDatasetMetric(dset_tst_pred,\n",
    "                                    unprivileged_groups, privileged_groups)\n",
    "    print(\"PREROLLED METRICS\")\n",
    "    print(metric_tst.num_positives(privileged = True) / metric_tst.num_instances(privileged = True))\n",
    "    print(metric_tst.num_positives(privileged = False) / metric_tst.num_instances(privileged = False))\n",
    "    print(\"Disparate impact is %0.2f (closer to 1 is better)\" % metric_tst.disparate_impact())\n",
    "    print(\"Mean difference  is %0.2f (closer to 0 is better)\" % metric_tst.mean_difference())\n",
    "    \n",
    "    return lmod, dset_tst_pred, metric_tst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "sup_lmod, sup_pred, sup_metric = build_logit_model_suppression(dset_raw_trn, dset_raw_tst, priv_group, unpriv_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression turns out not to be that bad, in the sense that the mean difference is reduced compared to the baseline model presented above and disparate impact is closer to 1. This result is referenced on p 117 but not adequately discussed there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing via reweighting\n",
    "### p 117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "np.random.seed(316)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p 120\n",
    "## transform the data set\n",
    "RW = Reweighing(unprivileged_groups = unpriv_group,\n",
    "                privileged_groups   = priv_group)\n",
    "RW.fit(dset_raw_trn)\n",
    "dset_rewgt_trn = RW.transform(dset_raw_trn)\n",
    "\n",
    "## calculate the metric of interest\n",
    "metric_rewgt_trn = BinaryLabelDatasetMetric(dset_rewgt_trn, \n",
    "                                         unprivileged_groups = unpriv_group,\n",
    "                                         privileged_groups   = priv_group)\n",
    "print(\"Difference in mean outcomes = %f\" %\n",
    "      metric_rewgt_trn.mean_difference())\n",
    "print(\"Disparate impact = %f\" %\n",
    "      metric_rewgt_trn.disparate_impact())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4 weights resulte because there are 4 types\n",
    "## privileged/unprivileged x positive/negative outcome (2 x 2 = 4)\n",
    "set(dset_rewgt_trn.instance_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have reweighted the data, fit a logistic regression with the reweighted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "## fairness preprocessed data\n",
    "rewgt_lmod, rewgt_pred, rewgt_metric = build_logit_model(dset_rewgt_trn, dset_raw_tst, priv_group, unpriv_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dpending on your fairness metric, this does slightly better than suppression as indicated by disparate impact. note that the disparate impact however would still meet the criterion of presumptive disparate impact under the EEOC's 4/5 rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(rewgt_lmod.coef_[0], raw_lmod.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see a difference in how the gender variable is weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gender is the second coefficient and here we plot the difference\n",
    "## between the coefficients in the reweighted as compared to raw/naive model\n",
    "plt.plot(rewgt_lmod.coef_[0] -  raw_lmod.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a scatetr plot shows that the coefficients are mostly quite close\n",
    "## in value in the 2 models\n",
    "plt.scatter(rewgt_lmod.coef_[0],  raw_lmod.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importantly we also see that the reweighting doesn't distort the values of the other coefficients\n",
    "## you should be asking whether this would be true if other variables were highly correlated with the\n",
    "## protected group. we will study this later in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at how the models treat females vs. males in the case of the model trained on the raw data and on the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem_idx = np.where(dset_raw_tst.features[:, 1] == 0)[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem_test_case = np.copy(dset_raw_tst.features[fem_idx:(fem_idx + 1)]) ## funny slicing to preserve 2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem_test_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewgt_lmod.predict_proba(fem_test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_lmod.predict_proba(fem_test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_male_test_case = np.copy(fem_test_case)\n",
    "fake_male_test_case[0, 1] = 1.0\n",
    "fake_male_test_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewgt_lmod.predict_proba(fake_male_test_case) - rewgt_lmod.predict_proba(fem_test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_lmod.predict_proba(fake_male_test_case) - raw_lmod.predict_proba(fem_test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_lmod.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So probability of being successful goes up by almost 10% just for being male in the raw case even with all else being equal. this looks like an unfair/illegal model (but then again unlikely)  such a model would get deployed in a legally regulated area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning fair representations\n",
    "### p 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "TR = LFR(unprivileged_groups = unpriv_group, \n",
    "         privileged_groups = priv_group)\n",
    "TR = TR.fit(dset_raw_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_lfr_trn = TR.transform(dset_raw_trn, thresh = 0.5)\n",
    "dset_lfr_trn = dset_raw_trn.align_datasets(dset_lfr_trn)\n",
    "\n",
    "dset_lfr_tst = TR.transform(dset_raw_tst, thresh = 0.5)\n",
    "dset_lfr_tst = dset_raw_trn.align_datasets(dset_lfr_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_op = BinaryLabelDatasetMetric(dset_lfr_trn, \n",
    "                                      unprivileged_groups = unpriv_group,\n",
    "                                      privileged_groups   = priv_group)\n",
    "print(\"Mean difference:  %0.2f\" % metric_op.mean_difference())\n",
    "print(\"Disparate impact: %0.2f\" % metric_op.disparate_impact())\n",
    "print(\"Size %d\" % dset_lfr_trn.features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_op_tst = BinaryLabelDatasetMetric(dset_lfr_tst, \n",
    "                                      unprivileged_groups = unpriv_group,\n",
    "                                      privileged_groups   = priv_group)\n",
    "print(\"Mean difference:  %0.2f\" % metric_op_tst.mean_difference())\n",
    "print(\"Disparate impact: %0.2f\" % metric_op_tst.disparate_impact())\n",
    "print(\"Size %d\" % dset_lfr_tst.features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "## fairness preprocessed data\n",
    "lfr_lmod1, lfr_pred, lfr_metric = build_logit_model(dset_lfr_trn, dset_raw_tst, priv_group, unpriv_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning additional hyperparameters\n",
    "#### not covered in book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for thresh in thresholds:\n",
    "    \n",
    "    # Transform training data and align features\n",
    "    dset_lfr_trn = TR.transform(dset_raw_trn, threshold = thresh)\n",
    "\n",
    "    metric_lfr_trn = BinaryLabelDatasetMetric(dset_lfr_trn, \n",
    "                                             unprivileged_groups = unpriv_group,\n",
    "                                             privileged_groups   = priv_group)\n",
    "\n",
    "    unpriv_idx = np.where(dset_lfr_trn.protected_attributes.ravel() == 0.0)[0]\n",
    "    print(\"Pct of positive outcomes for unpriv group: %0.3f\" % \n",
    "          (np.where(dset_lfr_trn.labels[unpriv_idx] == 1.0)[0].shape[0] / unpriv_idx.shape[0]))\n",
    "    \n",
    "    priv_idx = np.where(dset_lfr_trn.protected_attributes.ravel() == 1.0)[0]\n",
    "    print(\"Pct of positive outcomes for priv group: %0.3f\\n\" % \n",
    "          (np.where(dset_lfr_trn.labels[priv_idx] == 1.0)[0].shape[0] / priv_idx.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing does not remove the potential for in-processing or post-processing interventions. More on this in the next two chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For example, consider whether to retain the logistic regression classification threshold at 0.5\n",
    "#### even after the data has been transformed. Perhaps it should be shifted to increase accuracy \n",
    "#### in recognition that some accuracy has been sacrificed in transforming the data, maybe it can be\n",
    "#### recovered in optimizing the threshold as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Also we have only compared along group fairness metrics. We should also consider incorporating measures of\n",
    "#### individual fairness. However this would be a poor example to consider individual fairness\n",
    "#### given that the categories are so broad (age crossed with education). The less specific the data\n",
    "#### the less compelling the interest in individual fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess by learning an optimal representation\n",
    "### p 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p 129\n",
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "optim_options = {\n",
    "    \"distortion_fun\": get_distortion_adult,\n",
    "    \"epsilon\": 0.05,\n",
    "    \"clist\": [0.99, 1.99, 2.99],\n",
    "    \"dlist\": [.1, 0.05, 0]\n",
    "}\n",
    "\n",
    "OP = OptimPreproc(OptTools, optim_options)\n",
    "\n",
    "OP = OP.fit(dset_raw_trn)\n",
    "\n",
    "## p 131\n",
    "# Transform training data and align features\n",
    "dset_op_trn = OP.transform(dset_raw_trn, transform_Y=True)\n",
    "dset_op_trn = dset_raw_trn.align_datasets(dset_op_trn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_op = BinaryLabelDatasetMetric(dset_op_trn, \n",
    "                                      unprivileged_groups = unpriv_group,\n",
    "                                      privileged_groups   = priv_group)\n",
    "print(\"Mean difference:  %0.2f\" % metric_op.mean_difference())\n",
    "print(\"Disparate impact: %0.2f\" % metric_op.disparate_impact())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform testing data\n",
    "dset_op_tst = OP.transform(dset_raw_tst, transform_Y=True)\n",
    "dset_op_tst = dset_raw_trn.align_datasets(dset_op_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "## fairness preprocessed data\n",
    "op_lmod, op_pred, op_metric = build_logit_model(dset_op_trn, dset_op_tst, priv_group, unpriv_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can define the distortion function/metric differently e.g. based on domain knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p 132\n",
    "def get_distortion_adult2(vold, vnew):\n",
    "    # Define local functions to adjust education and age\n",
    "    def adjustEdu(v):\n",
    "        if v == '>12':\n",
    "            return 13\n",
    "        elif v == '<6':\n",
    "            return 5\n",
    "        else:\n",
    "            return int(v)\n",
    "\n",
    "    def adjustAge(a):\n",
    "        if a == '>=70':\n",
    "            return 70.0\n",
    "        else:\n",
    "            return float(a)\n",
    "\n",
    "    def adjustInc(a):\n",
    "        if a == \"<=50K\":\n",
    "            return 0\n",
    "        elif a == \">50K\":\n",
    "            return 1\n",
    "        else:\n",
    "            return int(a)\n",
    "\n",
    "    # value that will be returned for events that should not occur\n",
    "    bad_val = 3.0\n",
    "\n",
    "    # Adjust education years\n",
    "    eOld = adjustEdu(vold['Education Years'])\n",
    "    eNew = adjustEdu(vnew['Education Years'])\n",
    "\n",
    "    # Education cannot be lowered or increased in more than 1 year\n",
    "    #########################################################################\n",
    "    if (eNew < eOld - 1) | (eNew > eOld+1): ## CHANGED THIS TO LESS STRINGENT\n",
    "        return bad_val\n",
    "    #########################################################################\n",
    "    # adjust age\n",
    "    aOld = adjustAge(vold['Age (decade)'])\n",
    "    aNew = adjustAge(vnew['Age (decade)'])\n",
    "\n",
    "    # Age cannot be increased or decreased in more than a decade\n",
    "    #########################################################################\n",
    "    if np.abs(aOld-aNew) > 15.0: ## CHANGED THIS TO LESS STRINGENT\n",
    "        return bad_val\n",
    "    #########################################################################\n",
    "\n",
    "    # Penalty of 2 if age is decreased or increased\n",
    "    if np.abs(aOld-aNew) > 0:\n",
    "        return 2.0\n",
    "\n",
    "    # Adjust income\n",
    "    incOld = adjustInc(vold['Income Binary'])\n",
    "    incNew = adjustInc(vnew['Income Binary'])\n",
    "\n",
    "    # final penalty according to income\n",
    "    if incOld > incNew:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "optim_options2 = {\n",
    "    \"distortion_fun\": get_distortion_adult2,\n",
    "    \"epsilon\": 0.05,\n",
    "    \"clist\": [0.99, 1.99, 2.99],\n",
    "    \"dlist\": [.1, 0.05, 0]\n",
    "}\n",
    "\n",
    "OP2 = OptimPreproc(OptTools, optim_options2)\n",
    "OP2 = OP2.fit(dset_raw_trn)\n",
    "\n",
    "# Transform training data and align features\n",
    "dset_op_trn2 = OP2.transform(dset_raw_trn, transform_Y=True)\n",
    "dset_op_trn2 = dset_raw_trn.align_datasets(dset_op_trn2)\n",
    "\n",
    "metric_op2 = BinaryLabelDatasetMetric(dset_op_trn2, \n",
    "                                      unprivileged_groups = unpriv_group,\n",
    "                                      privileged_groups   = priv_group)\n",
    "print(\"Mean difference:  %0.2f\" % metric_op2.mean_difference())\n",
    "print(\"Disparate impact: %0.2f\" % metric_op2.disparate_impact())\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "## fairness preprocessed data\n",
    "op_lmod2, op_pred2, op_metric2 = build_logit_model(dset_op_trn2, dset_raw_tst, priv_group, unpriv_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternately can adjust the tolerance by upping the probability limits for the distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "optim_options3 = {\n",
    "    \"distortion_fun\": get_distortion_adult,\n",
    "    \"epsilon\": 0.05,\n",
    "    \"clist\": [0.99, 1.99, 2.99],\n",
    "    \"dlist\": [.15, 0.10, 0.05]\n",
    "}\n",
    "\n",
    "OP3 = OptimPreproc(OptTools, optim_options)\n",
    "\n",
    "OP3 = OP.fit(dset_raw_trn)\n",
    "\n",
    "# Transform training data and align features\n",
    "dset_op_trn3 = OP3.transform(dset_raw_trn, transform_Y=True)\n",
    "dset_op_trn3 = dset_raw_trn.align_datasets(dset_op_trn3)\n",
    "\n",
    "metric_op3 = BinaryLabelDatasetMetric(dset_op_trn3, \n",
    "                                      unprivileged_groups = unpriv_group,\n",
    "                                      privileged_groups   = priv_group)\n",
    "print(\"Mean difference:  %0.2f\" % metric_op3.mean_difference())\n",
    "print(\"Disparate impact: %0.2f\" % metric_op3.disparate_impact())\n",
    "\n",
    "## Transform testing data\n",
    "dset_op_tst3 = OP.transform(dset_raw_tst, transform_Y=True)\n",
    "dset_op_tst3 = dset_raw_trn.align_datasets(dset_op_tst3)\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(316)\n",
    "\n",
    "## fairness preprocessed data\n",
    "op_lmod3, op_pred3, op_metric3 = build_logit_model(dset_op_trn3, dset_op_tst3, priv_group, unpriv_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note we don't need to transform the test data to enjoy the benefits of the transformation\n",
    "op_lmod4, op_pred4, op_metric4 = build_logit_model(dset_op_trn3, dset_raw_tst, priv_group, unpriv_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question for ourselves: When might we find larger deviations/distortions acceptable or less acceptable when considering a potential accuracy/fairness trade off (which need not necessarily exist)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## when accuracy is not of prime importance (e.g. low stakes consumer decisions)\n",
    "## don't do that on health decisions!\n",
    "## or when we think the data is probably noisy anyway so that we may not be adding much\n",
    "## noise compared to original data collection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
